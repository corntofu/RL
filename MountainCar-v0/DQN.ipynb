{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjozQD67Cboi",
        "outputId": "7286d4c3-828d-4ac8-d5b1-377c022cdbbc"
      },
      "outputs": [],
      "source": [
        "%pip install swig\n",
        "%pip install gymnasium[box2d]\n",
        "%pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlq3xemmmZI3"
      },
      "source": [
        "# PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8h-pWtQytuU"
      },
      "outputs": [],
      "source": [
        "# ppo_gae_minibatch.py\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal, Categorical\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import time # Import time for rendering delay\n",
        "import imageio # Import imageio for GIF creation\n",
        "\n",
        "# Ensure imageio is installed\n",
        "try:\n",
        "    import imageio\n",
        "except ImportError:\n",
        "    print(\"imageio not found. Installing imageio...\")\n",
        "    %pip install imageio\n",
        "    import imageio\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------------------------\n",
        "# Rollout Buffer (for GAE)\n",
        "# ---------------------------\n",
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.log_probs = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.state_values = []\n",
        "\n",
        "    def clear(self):\n",
        "        self.__init__()\n",
        "\n",
        "    def to_tensors(self):\n",
        "        # stack and move to device\n",
        "        states = torch.stack(self.states).to(device)  # (T, state_dim)\n",
        "        actions = torch.stack(self.actions).to(device)\n",
        "        log_probs = torch.stack(self.log_probs).to(device)\n",
        "        state_values = torch.stack(self.state_values).to(device)\n",
        "        rewards = torch.tensor(self.rewards, dtype=torch.float32).to(device)\n",
        "        dones = torch.tensor(self.dones, dtype=torch.float32).to(device)\n",
        "        return states, actions, log_probs, state_values, rewards, dones\n",
        "\n",
        "# ---------------------------\n",
        "# ActorCritic\n",
        "# ---------------------------\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init=0.6):\n",
        "        super().__init__()\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        # shared or separate nets? we keep separate heads for clarity\n",
        "        # actor\n",
        "        if has_continuous_action_space:\n",
        "            self.actor = nn.Sequential(\n",
        "                nn.Linear(state_dim, 64),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(64, 64),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(64, action_dim),\n",
        "                nn.Tanh()  # outputs in [-1,1], scale externally as needed\n",
        "            )\n",
        "            # store action variance (as tensor of variances)\n",
        "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
        "        else:\n",
        "            self.actor = nn.Sequential(\n",
        "                nn.Linear(state_dim, 64),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(64, 64),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(64, action_dim),\n",
        "                nn.Softmax(dim=-1)\n",
        "            )\n",
        "\n",
        "        # critic\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "        if not self.has_continuous_action_space:\n",
        "            print(\"Warning: trying to set action std for discrete action space.\")\n",
        "            return\n",
        "        self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        state: tensor (state_dim,) or (1, state_dim) -> returns action, logprob, value\n",
        "        \"\"\"\n",
        "        if state.dim() == 1:\n",
        "            state = state.unsqueeze(0)  # -> (1, state_dim)\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)  # (B, action_dim)\n",
        "            action_var = self.action_var.expand_as(action_mean)\n",
        "            cov_mat = torch.diag_embed(action_var)  # (B, action_dim, action_dim)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        state_val = self.critic(state).squeeze(-1)  # (B,)\n",
        "\n",
        "        # squeeze if batch size 1\n",
        "        return action.squeeze(0), action_logprob.squeeze(0), state_val.squeeze(0)\n",
        "\n",
        "    def evaluate(self, states, actions):\n",
        "        \"\"\"\n",
        "        states: (N, state_dim)\n",
        "        actions: (N, ...) - shape depends on discrete/continuous\n",
        "        returns: logprobs (N,), values (N,), entropy (N,)\n",
        "        \"\"\"\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(states)\n",
        "            action_var = self.action_var.expand_as(action_mean)\n",
        "            cov_mat = torch.diag_embed(action_var)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        else:\n",
        "            action_probs = self.actor(states)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(actions)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(states).squeeze(-1)\n",
        "        return action_logprobs, state_values, dist_entropy\n",
        "\n",
        "# ---------------------------\n",
        "# PPO with GAE + Minibatch\n",
        "# ---------------------------\n",
        "class PPO:\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim,\n",
        "        action_dim,\n",
        "        has_continuous_action_space,\n",
        "        lr_actor=3e-4,\n",
        "        lr_critic=1e-3,\n",
        "        gamma=0.99,\n",
        "        K_epochs=10,\n",
        "        eps_clip=0.2,\n",
        "        gae_lambda=0.95,\n",
        "        action_std_init=0.6,\n",
        "        entropy_coef=0.01,\n",
        "        max_grad_norm=0.5,\n",
        "        minibatch_size=64,\n",
        "        device=device\n",
        "    ):\n",
        "        self.device = device\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.minibatch_size = minibatch_size\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(self.device)\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(self.device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # separate optimizers for actor and critic (we pass actor params and critic params)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "            {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "            {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "        ])\n",
        "\n",
        "        self.buffer = RolloutBuffer()\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "        if self.has_continuous_action_space:\n",
        "            self.policy.set_action_std(new_action_std)\n",
        "            self.policy_old.set_action_std(new_action_std)\n",
        "        else:\n",
        "            print(\"Warning: discrete action space, ignoring set_action_std.\")\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"\n",
        "        state: numpy array from env\n",
        "        returns: action in env-friendly format (numpy scalar/array)\n",
        "        and internally stores tensors in buffer\n",
        "        \"\"\"\n",
        "        state_t = torch.FloatTensor(state).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            action, action_logprob, state_val = self.policy_old.act(state_t)\n",
        "\n",
        "        # store tensors (ensure consistent shapes)\n",
        "        self.buffer.states.append(state_t)\n",
        "        # actions: ensure actions stored as tensor (for discrete it's scalar tensor)\n",
        "        self.buffer.actions.append(action.detach())\n",
        "        self.buffer.log_probs.append(action_logprob.detach())\n",
        "        self.buffer.state_values.append(state_val.detach())\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            return action.cpu().numpy()\n",
        "        else:\n",
        "            return action.item()\n",
        "\n",
        "    def compute_gae(self, rewards, dones, values, next_value):\n",
        "        \"\"\"\n",
        "        rewards, dones, values: all torch tensors (1D) of length T\n",
        "        next_value: scalar tensor (value at t+1)\n",
        "        returns: advantages (T,), returns (T,) (i.e., targets for critic)\n",
        "        \"\"\"\n",
        "        T = len(rewards)\n",
        "        advantages = torch.zeros(T, dtype=torch.float32).to(self.device)\n",
        "        last_gae = 0.0\n",
        "        for t in reversed(range(T)):\n",
        "            mask = 1.0 - dones[t]  # 0 if done, 1 if not done\n",
        "            delta = rewards[t] + self.gamma * next_value * mask - values[t]\n",
        "            last_gae = delta + self.gamma * self.gae_lambda * mask * last_gae\n",
        "            advantages[t] = last_gae\n",
        "            next_value = values[t]\n",
        "        returns = advantages + values\n",
        "        return advantages, returns\n",
        "\n",
        "    def update(self):\n",
        "        # convert buffer to tensors\n",
        "        states, actions, old_log_probs, old_values, rewards, dones = self.buffer.to_tensors()\n",
        "\n",
        "        # compute next_value for last step (bootstrap)\n",
        "        with torch.no_grad():\n",
        "            # last state's value predicted by old policy (if buffer empty this will error upstream)\n",
        "            if len(self.buffer.states) == 0:\n",
        "                return\n",
        "            last_state = self.buffer.states[-1]\n",
        "            next_value = self.policy_old.critic(last_state.unsqueeze(0)).squeeze(0).detach()\n",
        "            # if last step was terminal, next_value should be 0 -> handled in compute_gae via dones mask\n",
        "\n",
        "        # compute advantages & returns via GAE\n",
        "        advantages, returns = self.compute_gae(rewards, dones, old_values.squeeze(-1), next_value)\n",
        "\n",
        "        # normalize advantages\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        # flatten tensors: states (T, state_dim), actions shape depends, log_probs (T,)\n",
        "        T = states.shape[0]\n",
        "        batch_size = T\n",
        "        # Prepare indices for minibatching\n",
        "        indices = np.arange(batch_size)\n",
        "\n",
        "        # convert returns to same shape as values for MSE\n",
        "        returns = returns.detach()\n",
        "\n",
        "        # repeat K_epochs times with minibatches\n",
        "        for epoch in range(self.K_epochs):\n",
        "            np.random.shuffle(indices)\n",
        "            for start in range(0, batch_size, self.minibatch_size):\n",
        "                end = start + self.minibatch_size\n",
        "                mb_idx = indices[start:end]\n",
        "\n",
        "                mb_states = states[mb_idx]\n",
        "                mb_actions = actions[mb_idx]\n",
        "                mb_old_log_probs = old_log_probs[mb_idx]\n",
        "                mb_returns = returns[mb_idx]\n",
        "                mb_advantages = advantages[mb_idx]\n",
        "\n",
        "                # evaluate current policy\n",
        "                logprobs, state_values, dist_entropy = self.policy.evaluate(mb_states, mb_actions)\n",
        "                # Ensure shapes\n",
        "                state_values = state_values.view(-1)\n",
        "\n",
        "                # ratio for PPO\n",
        "                ratios = torch.exp(logprobs - mb_old_log_probs.detach())\n",
        "\n",
        "                surr1 = ratios * mb_advantages\n",
        "                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * mb_advantages\n",
        "                loss_actor = -torch.min(surr1, surr2).mean()\n",
        "                loss_critic = self.mse_loss(state_values, mb_returns)\n",
        "                loss_entropy = -self.entropy_coef * dist_entropy.mean()\n",
        "\n",
        "                loss = loss_actor + loss_critic + loss_entropy\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                # gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
        "                self.optimizer.step()\n",
        "\n",
        "        # copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        # clear buffer\n",
        "        self.buffer.clear()\n",
        "\n",
        "# ---------------------------\n",
        "# Training loop (CartPole example)\n",
        "# ---------------------------\n",
        "def train_cartpole(\n",
        "    env_name=\"MountainCar-v0\",\n",
        "    max_updates=1000,\n",
        "    max_steps_per_update=2048,\n",
        "    update_epochs=10,\n",
        "    minibatch_size=64,\n",
        "    render=False,\n",
        "    seed=42\n",
        "):\n",
        "    # seed\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "\n",
        "    # Determine if action space is continuous or discrete\n",
        "    if isinstance(env.action_space, gym.spaces.Box):\n",
        "        action_dim = env.action_space.shape[0] # For continuous action spaces\n",
        "        is_continuous = True\n",
        "    elif isinstance(env.action_space, gym.spaces.Discrete):\n",
        "        action_dim = env.action_space.n # For discrete action spaces\n",
        "        is_continuous = False\n",
        "    else:\n",
        "        raise NotImplementedError(\"Unsupported action space type\")\n",
        "\n",
        "    ppo = PPO(\n",
        "        state_dim=state_dim,\n",
        "        action_dim=action_dim,\n",
        "        has_continuous_action_space=is_continuous,\n",
        "        lr_actor=3e-4,\n",
        "        lr_critic=1e-3,\n",
        "        gamma=0.99,\n",
        "        K_epochs=update_epochs,\n",
        "        eps_clip=0.2,\n",
        "        gae_lambda=0.95,\n",
        "        action_std_init=0.6,\n",
        "        entropy_coef=0.01,\n",
        "        max_grad_norm=0.5,\n",
        "        minibatch_size=minibatch_size,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    running_reward = 0\n",
        "    avg_length = 0\n",
        "    time_step = 0\n",
        "    scores_deque = deque(maxlen=100)\n",
        "\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    state = obs\n",
        "    penalty = mn = mx = 0\n",
        "    for update in range(1, max_updates + 1):\n",
        "        # collect trajectories until we have max_steps_per_update transitions\n",
        "        for step in range(max_steps_per_update):\n",
        "            action = ppo.select_action(state)\n",
        "\n",
        "            # step environment\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # store reward and done\n",
        "            ppo.buffer.rewards.append(reward)\n",
        "            ppo.buffer.dones.append(float(done))\n",
        "\n",
        "            state = next_state\n",
        "            time_step += 1\n",
        "            running_reward += reward\n",
        "            avg_length += 1\n",
        "\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            if done:\n",
        "                if terminated:\n",
        "                    print(\"=========================================\")\n",
        "                    print(\"         Reached To The Top!!!!!!!       \")\n",
        "                    print(\"=========================================\")\n",
        "\n",
        "                scores_deque.append(running_reward)\n",
        "                obs, info = env.reset()\n",
        "                state = obs\n",
        "                avg_length = 0\n",
        "                penalty = 0\n",
        "\n",
        "        # After collecting enough samples, call update\n",
        "        ppo.update()\n",
        "\n",
        "        # logging\n",
        "        if len(scores_deque) > 0:\n",
        "            avg_score = np.mean(scores_deque)\n",
        "        else:\n",
        "            avg_score = running_reward / (update * max_steps_per_update)\n",
        "\n",
        "        print(f\"Update {update}\\tAverage100EpLen {avg_score:.2f}\\tRunning reward (last batch sum) {np.mean(scores_deque)}\")\n",
        "        # stopping condition (CartPole solved ~195 over 100 episodes)\n",
        "        if np.mean(scores_deque) >= 50:\n",
        "            print(\"Environment solved!\")\n",
        "            break\n",
        "\n",
        "        running_reward = 0\n",
        "\n",
        "    env.close()\n",
        "    return ppo\n",
        "\n",
        "def demonstrate_policy(ppo_agent, env_name=\"CartPole-v1\", num_episodes=5, seed=42, save_gif=False, gif_filename=\"policy_demonstration.gif\", gif_fps=30):\n",
        "    \"\"\"Demonstrates the trained PPO agent in the environment with rendering, and optionally saves a GIF.\"\"\"\n",
        "    print(f\"\\nDemonstrating policy for {env_name}...\")\n",
        "\n",
        "    render_mode = \"rgb_array\" if save_gif else \"human\"\n",
        "    eval_env = gym.make(env_name, render_mode=render_mode)\n",
        "\n",
        "    all_frames = []\n",
        "\n",
        "    # If saving GIF, only run for 1 episode to keep file size manageable\n",
        "    episodes_to_run = 1 if save_gif else num_episodes\n",
        "\n",
        "    for episode in range(episodes_to_run):\n",
        "        obs, info = eval_env.reset(seed=seed + episode)\n",
        "        state = obs\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        print(f\"--- Episode {episode + 1} ---\")\n",
        "        while not done:\n",
        "            action = ppo_agent.select_action(state)\n",
        "            if abs(action) < 0.33:\n",
        "                action = 1\n",
        "            elif action >= 0.33:\n",
        "                action = 2\n",
        "            else:\n",
        "                action = 0\n",
        "            next_state, reward, terminated, truncated, info = eval_env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if save_gif:\n",
        "                frame = eval_env.render()\n",
        "                all_frames.append(frame)\n",
        "            else: # Human render mode\n",
        "                eval_env.render()\n",
        "                time.sleep(0.01) # Small delay for better visualization\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            step_count += 1\n",
        "        print(f\"Episode finished after {step_count} steps with total reward: {total_reward}\")\n",
        "\n",
        "    eval_env.close()\n",
        "    print(\"Demonstration complete.\")\n",
        "\n",
        "    if save_gif and len(all_frames) > 0:\n",
        "        print(f\"Saving GIF to {gif_filename}...\")\n",
        "        imageio.mimsave(gif_filename, all_frames, fps=gif_fps)\n",
        "        print(\"GIF saved.\")\n",
        "\n",
        "    return all_frames # Return frames if needed elsewhere (though not explicitly requested, good for flexibility)\n",
        "\n",
        "env_train = \"MountainCarContinuous-v0\"\n",
        "env_simul = 'MountainCar-v0'\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 실행 예시: (터미널에서) python ppo_gae_minibatch.py\n",
        "    trained_ppo = train_cartpole(\n",
        "        env_name=env_train,\n",
        "        max_updates=200,\n",
        "        max_steps_per_update=2048,\n",
        "        update_epochs=10,\n",
        "        minibatch_size=64,\n",
        "        render=False,\n",
        "        seed=123\n",
        "    )\n",
        "\n",
        "    # Demonstrate the trained policy and save GIF\n",
        "    demonstrate_policy(trained_ppo, env_name=env_simul, num_episodes=1, seed=123, save_gif=True, gif_filename=\"Cart_ppo_demo.gif\")\n",
        "\n",
        "    # If you still want to see the human rendering for multiple episodes, you can call it again\n",
        "    # demonstrate_policy(trained_ppo, env_name=\"CartPole-v1\", num_episodes=3, seed=123, save_gif=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FJVpWf5W9jQ"
      },
      "source": [
        "# PPO(SB3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM7zWbyNjJJW",
        "outputId": "c534c9f7-4b9b-411d-ed3f-c0b25981a503"
      },
      "outputs": [],
      "source": [
        "%pip install shimmy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgZ-lU92XKGK",
        "outputId": "46d949f4-b2ac-4b93-b097-94eb27a6d482"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "dqn = DQN(\n",
        "    \"MlpPolicy\",\n",
        "    env=env,\n",
        "    verbose=1,\n",
        "    batch_size=256,\n",
        "    gamma=0.99,\n",
        "    learning_rate=0.0004,\n",
        "    exploration_initial_eps=1,\n",
        "    exploration_final_eps=0.2,\n",
        "    exploration_fraction=0.3\n",
        ")\n",
        "\n",
        "dqn.learn(total_timesteps=3000000, log_interval=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-0I4yRNY_-k"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import time\n",
        "import imageio\n",
        "from stable_baselines3.common.base_class import BaseAlgorithm # Import to check SB3 agent type\n",
        "\n",
        "def demonstrate_policy(agent, env_name=\"CartPole-v1\", num_episodes=5, seed=42, save_gif=False, gif_filename=\"policy_demonstration.gif\", gif_fps=30):\n",
        "    \"\"\"\n",
        "    Demonstrates a trained RL agent in an environment with rendering, and optionally saves a GIF.\n",
        "    This function is adapted to work with both custom agents (having 'select_action')\n",
        "    and Stable-Baselines3 agents (having 'predict').\n",
        "    \"\"\"\n",
        "    print(f\"\\nDemonstrating policy for {env_name}...\")\n",
        "\n",
        "    render_mode = \"rgb_array\" if save_gif else \"human\"\n",
        "    eval_env = gym.make(env_name, render_mode=render_mode)\n",
        "\n",
        "    all_frames = []\n",
        "\n",
        "    # If saving GIF, only run for 1 episode to keep file size manageable\n",
        "    episodes_to_run = 1 if save_gif else num_episodes\n",
        "\n",
        "    # Check if the agent is a Stable-Baselines3 agent\n",
        "    is_sb3_agent = isinstance(agent, BaseAlgorithm)\n",
        "\n",
        "    for episode in range(episodes_to_run):\n",
        "        obs, info = eval_env.reset(seed=seed + episode)\n",
        "        state = obs\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        print(f\"--- Episode {episode + 1} ---\")\n",
        "        while not done:\n",
        "            action = None\n",
        "            if is_sb3_agent:\n",
        "                # Stable-Baselines3 agents use .predict()\n",
        "                action, _states = agent.predict(state, deterministic=True)\n",
        "                # For discrete action spaces (like MountainCar-v0 with DQN), action is already the correct integer.\n",
        "                # No further mapping needed.\n",
        "            elif hasattr(agent, 'select_action'):\n",
        "                # Custom PPO-like agents might have .select_action()\n",
        "                action = agent.select_action(state)\n",
        "                # This specific mapping was used in the PPO example for MountainCar-v0 simulation\n",
        "                # when the PPO agent was trained on continuous MountainCarContinuous.\n",
        "                # This logic is applied only for that specific case.\n",
        "                if env_name == \"MountainCar-v0\" and hasattr(agent, 'has_continuous_action_space') and agent.has_continuous_action_space:\n",
        "                    action = np.argmax(action)\n",
        "            else:\n",
        "                raise AttributeError(\"Agent must have 'select_action' or be a Stable-Baselines3 agent with 'predict' method.\")\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = eval_env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if save_gif:\n",
        "                frame = eval_env.render()\n",
        "                all_frames.append(frame)\n",
        "            else: # Human render mode\n",
        "                eval_env.render()\n",
        "                time.sleep(0.01) # Small delay for better visualization\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            step_count += 1\n",
        "        print(f\"Episode finished after {step_count} steps with total reward: {total_reward}\")\n",
        "\n",
        "    eval_env.close()\n",
        "    print(\"Demonstration complete.\")\n",
        "\n",
        "    if save_gif and len(all_frames) > 0:\n",
        "        print(f\"Saving GIF to {gif_filename}...\")\n",
        "        imageio.mimsave(gif_filename, all_frames, fps=gif_fps)\n",
        "        print(\"GIF saved.\")\n",
        "\n",
        "    return all_frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikOUiMGJZfUG"
      },
      "outputs": [],
      "source": [
        "demonstrate_policy(env_name=\"MountainCar-v0\", agent = dqn, save_gif = True)\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aienv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
